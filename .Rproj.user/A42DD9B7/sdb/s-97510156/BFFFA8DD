{
    "contents" : "# lib-poks.R\nPToOdds <- function(p) p/(1-p)\nOddsToP <- function(o) o/(1+o)\nentropy.system <- function(p) {q=(1-p); -( sum((p * log(p)), na.rm=T) + sum(q * log(q), na.rm=T)); }\nentropy.vector <- function(p) {q=(1-p); r=-( p * log(p) + q * log(q) ); r[is.na(r)]=0;return(r); }\nodds.entropy <- function (o) - sum(((o*log(o)+log(1))/(1+o) - log(1+o)),na.rm=T)\nodds.entropy.vector <- function (o) {\n  r = - ((o*log(o)+log(1))/(1+o) - log(1+o))\n  r[is.na(r)]=0\n  return(r);\n}\nOdds.cmd <- function(d) {if(is.null(dim(d)))return(rep(1,length(d)));p <- (colSums(d,na.rm=T)+1)/(nrow(d)+2); p/(1-p);}\nOdds <- function (p) { p / (1 - p); }\nprodColumn <- function(m) if(is.matrix(m)) apply(m,2,prod) else { if(is.vector(m)) m else 1}\ncolSumsRobust <- function(x)  if(is.matrix(x)) colSums(x,na.rm=T) else {if(is.vector(x)) x else 0}\ncolMeans2 <- function(x,...) colMeans(colMeans(x,...),...)\n\nmissing.item <- -1    # no data available for a student over this item\nitem.presented <- NA       # item has been presented to student\nnot.answered <- -2    # item has been presented in data but student did not answer\n\n# example use: \n# raw <- matrix(scan('~/Kit/Simulation/Data/unix48.matrix'),48,34,byrow=T); dataset='Unix'\n# (ks <- ks.init(raw, alpha.c=.15, alpha.p=.2))\n# ks.update(1,F,ks$state,ks)\n# Note that item.presented in current.state indicates that these items were answered (observed) and they are no more estimated\nks.update <- function(item, val, current.state, ks) {\n  if(is.na(val) || is.na(current.state[item]) || item.missing(current.state[item]) || item.missing(val)) \n#    { multiplier <- 1 }                 # leave state unchanged for item.presented observed or item.presented in current state\n    {\n      current.state[item]=item.presented\n      return(current.state)\n    } # leave state unchanged for item.presented observed or item.presented in current state\n  else if(val > 0.5)\n    { multiplier <- (ks$or)$t[,item] } \n  else\n    { multiplier <- (ks$or)$f[,item] } \n\n  r <- current.state * multiplier\n  if(is.null(dim(r)))\n    r[item] <- item.presented\n  else\n    r[item,] <- item.presented\n  return(r)\n}\n\n#############################################################################\n# API functions\n#############################################################################\n\n# history: vector of answers before current question\n# q.id: name of current item in response vector\n# q.outcome: 0:failure, 1:success of q.no\n# ksarg: should normally not be necessary and will inherit the value given by config file\n# For example: POKSapi.new.answer(list(q22=1,q3=0,q1=1), 'q59',1,ks)\n# For example: POKSapi.new.answer(list(dinde2b=1,elflo7a=0), 'innut4',1,ks)\n# precompute: compute the items after the next, one for a success and one for a failure\n\npoks.api0 <- function(history, q.id, q.outcome, ksarg=ks, precompute=F) {\n  QuestionNames <- names(ks$state)\n  answers <- rep(item.presented, length(ks$state))\n  state.v <- unlist(history)\n  state.v <- state.v[intersect(names(history), names(ks$state))]\n  names(answers) <- QuestionNames\n  if(!is.null(state.v))\n    answers[names(state.v)] <- state.v\n  cur.state <- ks.update.vector(answers, ks$state, ks)\n  new.state <- ks.update(which(QuestionNames==q.id), q.outcome, cur.state, ks)\n  # traitement des doublons de racines\n  history.rac <- extract.racines(names(history))\n  if(!exists('QuestionNames.rac')) {\n    QuestionNames.rac <- extract.racines(QuestionNames)\n  }\n  Q.dup <- duplicated(c(history.rac, QuestionNames.rac))[(length(history.rac)+1):(length(history.rac)+length(QuestionNames.rac))]\n  new.state[Q.dup] <- NA\n  # fin racines\n  item.next <- getMostConnectedAndUncertainItem(new.state, ks)\n  if(precompute) {\n    next.stateT <- ks.update(item.next, 1, new.state, ks)\n    next.stateF <- ks.update(item.next, 0, new.state, ks)\n    item.next2T <- getMostConnectedAndUncertainItem(next.stateT, ks)\n    item.next2F <- getMostConnectedAndUncertainItem(next.stateF, ks)\n    return(c(item.next, item.next2T, item.next2F))\n  }\n  return(names(item.next))\n}\n\n\n# Wrapper to log into a file\npoks.api.wrap <- function(history, q.id, q.outcome, ksarg=ks, precompute=F, log.file=NULL, seed=NULL) {\n  if(! is.null(seed))\n    set.seed(seed)\n#  else\n#    set.seed(as.integer(format(Sys.time(), \"%s\")))\n  # Add q.id in history if not there\n  if(!is.element(q.id, names(history))) {\n    q.pair <- q.outcome\n    names(q.pair) <- q.id\n    history <- append(history, q.pair)\n  }\n  r <- poks.api1(history, q.id, q.outcome, ks, precompute)\n  if(!is.null(log.file) & is.character(log.file)) {\n    cat('>>', format(Sys.time(), '%s'), names(history), ';', q.id, ';', unlist(q.outcome), ';', r, '\\n', file=log.file, append=T)\n    if(is.element(r, append(names(history), q.id))) {\n      cat(format(Sys.time(), '%s'), 'doublon : ', r, '\\n', file=log.file, append=T)\n      browser()\n    }\n  }\n  return(r)\n}\n\n# version with fixed number of initial questions as determined by the length of nouvelles2010\n# 'history' must contain q.id already\npoks.api1 <- function(history, q.id, q.outcome, ksarg=ks, precompute=F) {\n  if(length(history) < (length(nouvelles2010) + length(first21)))\n    tryCatch(\n             {r <- poks.api.fixe(history, q.id, q.outcome, ksarg, precompute)},\n             error = function(e) {print(e);browser()}\n             )\n  else\n    tryCatch(\n             {\n               if(is.na(names(ks$state[q.id]))) { # like initial state\n                 q.id <- NA\n                 q.outcome <- NA\n               }\n               r <- poks.api0(history, q.id, q.outcome, ksarg, precompute)\n             },\n             error = function(e) {print(e);browser()}\n             )\n  return(r)\n}\n\npoks.api.fixe <- function(history, q.id, q.outcome, ksarg=ks, precompute=F) {\n  if(precompute) \n    n.items = 3\n  else\n    n.items = 1\n  if(length(history) < 30) {            # fixed item \n    if(length(history) < length(first21)) { # old items\n      not.asked <- setdiff(first21, names(history))\n      not.asked.prob <- first21.prob[not.asked]\n    } else {                            # new items\n      not.asked <- setdiff(nouvelles2010, names(history))\n      not.asked.prob <- nouvelles2010.prob[not.asked]\n    }\n    if(any(sapply(not.asked.prob, is.na)))\n      {cat('NA in prob', '\\n');browser(); }\n    if(length(not.asked) == 0)\n      {cat('no more items in fixed section', '\\n');browser(); }\n    tryCatch(\n             {\n               if(length(not.asked) < n.items)\n                 replace.flag <- TRUE\n               else\n                 replace.flag <- FALSE\n               r <- sample(not.asked, n.items, replace=replace.flag, prob=not.asked.prob)\n             },\n             error = function(e) {e;browser()}\n             )\n  } else {                              # adaptive after 31\n    stop('should not get here: fixed items api')\n  }\n  return(r)\n}  \n\nPOKSapi.new.answer <- poks.api.wrap\n\nPOKSapi.sequence.test <- function(nquest=60) {\n  sr <- sample(0:1, ncol(responses)+length(nouvelles2010), replace=T)\n  names(sr) <- c(colnames(responses), nouvelles2010)\n  sim.new.sequence(1, sr, ks)\n}\n\n# for testing: \nsim.new.sequence <- function(nruns, subj.real.resp, ks=NULL, precompute=F, ...) {\n  if(is.null(ks))\n    ks <- ks.init(responses, alpha.c=.25, alpha.p=.5)\n  q.new <- POKSapi.new.answer(list(), NA, NA, ks, precompute)\n  q.outcome <- subj.real.resp[q.new]\n  q.pair <- q.outcome\n  names(q.pair) <- q.new                # q.pair will be first in history\n  q.new <- POKSapi.new.answer(list(), q.new, q.outcome, ks, precompute)\n  history <- as.list(q.pair)\n  q.pair <- q.outcome\n  q.outcome <- subj.real.resp[q.new]\n  q.id <- q.new\n  # now we are ready to start the loop\n  for(i in 1:59) {\n    q.new <- POKSapi.new.answer(history, q.id, q.outcome, ks, precompute, ...)\n    # store q.id and its outcome in history\n    q.pair <- q.outcome\n    names(q.pair) <- q.id\n    history <- append(history, as.list(q.pair))\n    ## new outcome\n    q.outcome <- subj.real.resp[q.new]\n    if(is.na(q.outcome)) {               # maybe this fix should not be used\n      q.outcome <- sample(c(0,1),1 , prob=c(q.prob[q.new], 1-q.prob[q.new]))\n    }\n    q.id <- q.new\n    if(is.na(q.id)) {cat('q.id == NA', '\\n');browser(); }\n    if(is.na(q.outcome))  {cat('q.outcome == NA', '\\n'); browser(); }\n  }\n  return(unlist(history))\n}\n\n# Extracts Item types of Poly tests as defined by their names: first sequence of letters a single integer the rest are variants\nextract.racines <- function(lnoms) sapply(lnoms, function(nom) sub('([adegimtv][a-z]+[0-9]+).*','\\\\1', nom))\n\n\n# 'vector' is the response vector where:\n# 0: wrong answer, 1: good answer, item.presented, not.answered, missing.item\n# Note that item.presented in current.state indicates that these items were answered (observed) and they are no more estimated\nks.update.vector <- function(vector, current.state, ks) {\n  if(length(vector) != length(current.state)) {\n    stop('ks.update.vector: length of response and state vector should be the same')\n  }\n  state <- current.state\n  count = 1\n  for(i in vector) {\n    if(!is.na(match(i, c(0,1))))\n      state <- ks.update(count, i, state, ks)\n    count = 1 + count\n  }\n  return(state)\n}\n\n# Combine observed values with predicted and return probabilities instead of odds\nks.update.vector2 <- function(vector, current.state, ks) {\n  r <- OddsToP(ks.update.vector(vector, current.state, ks))\n  r[!is.na(vector)] <- vector[!is.na(vector)]\n  return(r)\n}\n\n# rawScores must be a respondant-item matrix; it can correspond to a snapshot of a\n# student assessment after x items; or it can be computed from raw (all responses).\n# Q-matrix names must be correct (Concepts X Items)\n# In the Q-matrix, missing values are considered 0.\n# Compute concept mastery based on a Q-matrix aggregation\ntopic.score <- function(rawScores, QMatrix, verbose=F) {\n  if(is.list(QMatrix)) {\n    r1 <- topic.score(rawScores, QMatrix[[1]], verbose)\n    QMatrix.rest <- QMatrix[-1]\n    if(length(QMatrix.rest) == 1)\n      return(topic.score(r1, QMatrix.rest[[1]], verbose))\n    else\n      return(topic.score(r1, QMatrix.rest, verbose))\n  }\n  if(is.null(dim(rawScores)))\n    rawScores <- t(rawScores)\n  commonItems <- intersect(colnames(rawScores), rownames(QMatrix))\n  if(verbose==T & length(commonItems) < length(colnames(rawScores)))\n    warning(cat('Missing items from Q-matrix:', setdiff(colnames(rawScores), commonItems), '\\n'))\n  if(verbose==T & length(commonItems) < length(rownames(QMatrix)))\n    warning(cat('Missing items from scores:', setdiff(rownames(QMatrix), commonItems), '\\n'))\n  rawScores[is.na(rawScores)] <- 0\n  QMatrix[is.na(QMatrix)] <- 0\n  return(rawScores[,commonItems] %*% QMatrix[commonItems,])\n}\n\n# Scores by all topics based on IRT\nltm.topic.scores <- function(global.ltm.model, topic.ltm.models, v.response, qmatrix3) {\n  if(!is.list(v.response))\n     v.response <- as.list(v.response)\n  r.t <- sapply(1:ncol(qmatrix3),\n              function(i) ltm.score(topic.ltm.models[[i]], v.response))\n  r <- c(ltm.score(global.ltm.model, as.list(v.response)), r.t)\n  names(r) <- c('Glob', colnames(qmatrix3))\n  return(r)\n}\n\n# Score based on an IRT model\n# ltm.model: result from ltm()\n# v.respon: either a vector congruent with ltm.model, or a list\n# correction: adjustment to make the average around 0.5 according to statistics 2010\n\nltm.score <- function(ltm.model, v.respon, correction=0.9) {\n  if(is.list(v.respon)) {\n    respon <- matrix(NA, nrow=1, ncol=nrow(ltm.model$coefficients))\n    names(respon) <- names(ltm.model$coefficients[,1])\n    ind <- intersect(names(v.respon), names(ltm.model$coefficients[,1]))\n    if(length(ind) == 0) {\n      return(NA)\n    }\n    respon[ind] <- unlist(v.respon)[ind]\n    respon <- matrix(respon, nrow=1)\n  }\n  else\n    respon <- matrix(v.respon, nrow=1)\n  r <- plogis(as.numeric(ltm::factor.scores(ltm.model, respon)$score.dat['z1'])+correction)\n  return(r)\n}\n\n# Use ltm model\nPOKSapi.topics <- function(scores, QMatrix=list(qmatrix, qmatrix2), matricule=NULL, write.to.file=NULL, verbose=F, robust=T, totalNItems=60, doNotKnow=0.25) {\n  ## responses.ltm and topic.ltm.models are ltm models loaded from ltm-models.R \n  r0 <- ltm.topic.scores(responses.ltm, topic.ltm.models, scores, qmatrix3)\n  r <- rep(doNotKnow, length(r0))*(totalNItems-length(scores))/totalNItems + r0*length(scores)/totalNItems\n  if(!is.null(write.to.file)) {\n    vscores <- matrix(NA, nrow=1, ncol=nrow(responses.ltm$coefficients))\n    colnames(vscores) <- rownames(responses.ltm$coefficients)\n    snames <- names(unlist(scores))\n    if(robust==T) {\n      subset.snames <- intersect(snames, colnames(vscores))\n      vscores[,subset.snames] <- (unlist(scores))[subset.snames]\n      if(length(subset.snames) != length(scores))\n        warning(cat('Score questions do not match with q-matrix\\nNumber of matches: ',length(subset.snames), '/', length(scores), '\\n'))\n    } else {\n      vscores[,snames] <- unlist(scores)\n    }\n    cat(matricule, format(Sys.time(), '%s'), vscores[,snames], snames, r, '\\n', file=write.to.file, append=T)\n  }\n  return(r)\n}\n\n# Initialize q-matrices (very specific to Poly pretest data)\nPOKSapi.get.qmatrix <- function(file='q-matrix.csv') as.matrix(read.csv(file,header=T,row.names=1))\nPOKSapi.qmatrix2 <- function(qmatrix) {\n  # First letter is category and we further collapse V(ector) and M(atrices)\n  collapsed.topics <- sub('V','M',sub('([A-Z]).*','\\\\1',colnames(qmatrix)))\n  unique.topics <- unique(collapsed.topics)\n  qmatrix2 <- sapply(unique.topics, function(i) (i == collapsed.topics))\n  rownames(qmatrix2) <- colnames(qmatrix)\n  colnames(qmatrix2) <- unique.topics\n  return(as.matrix(qmatrix2))\n}\n\n# eg. POKSapi.topics(list(elnde1a=1, elnde1b=1, ints2a=1,ditpr1=1,gadro8a=1,tutri2b=1,ditpr1=1,veups3=1,mauin1=1))\n# Scores is a list of the form <qname>=[0,1]\n# QMatrix is a single q-matrix or a list for which the transfer model is sequentially applied\nPOKSapi.topics.0 <- function(scores, QMatrix=list(qmatrix, qmatrix2), matricule=NULL, write.to.file=NULL, verbose=F, robust=T) {\n  vscores <- matrix(NA, nrow=1, ncol=nrow(qmatrix))\n  colnames(vscores) <- rownames(qmatrix)\n  snames <- names(unlist(scores))\n  if(robust==T) {\n    subset.snames <- intersect(snames, colnames(vscores))\n    vscores[,subset.snames] <- (unlist(scores))[subset.snames]\n    if(length(subset.snames) != length(scores))\n      warning(cat('Score questions do not match with q-matrix\\nNumber of matches: ',length(subset.snames), '/', length(scores), '\\n'))\n  }\n  else {\n    vscores[,snames] <- unlist(scores)\n  }\n  r <- competence.matrix.score(vscores, QMatrix, verbose)\n  if(!is.null(write.to.file))\n    cat(matricule, format(Sys.time(), '%s'), vscores[,snames], snames, r, '\\n', file=write.to.file, append=T)\n  return(r)\n}\n\n# Assumes missing values represent items not administered and non\n# answered items are score 0\n# QMatrix can be a list which represents successive transfer models\ncompetence.matrix.score <- function(rawScores, QMatrix, verbose=F) {\n  topic.score(rawScores, QMatrix, verbose=F) / topic.score(!is.na(rawScores), QMatrix, verbose=F)\n}\n\ngetExpEntropy <- function(state, ks) {\n  ks.q = length(state)\n  state.p <- OddsToP(state)\n  state.t <- sapply(1:ks.q, function(i) ks.update(i, 1, state, ks))\n  ent.t <- apply((state.t), 2, odds.entropy)\n  state.f <- sapply(1:ks.q, function(i) ks.update(i, 0, state, ks))\n  ent.f <- apply((state.f), 2, odds.entropy)\n  (ent.t * state.p) + (ent.f * (1 - state.p))\n}\n\n# state must be Odds\ngetHeuristicEntropy <- function(state, ks) {\n  state <- OddsToP(state)\n  state.ent <- entropy.vector(state)\n  q.ent.sum.t <- colSums(t(ks$m) * state.ent, na.rm=T)\n  q.ent.sum.f <- colSums(ks$m * state.ent, na.rm=T)\n  q.ent.exp <- (q.ent.sum.t * state) + (q.ent.sum.f * (1-state)) + sum(is.na(state))*state\n  return(q.ent.exp)\n}\n\n# state must be Odds\ngetHighestHeuristicEntropy <- function(state, ks) {\n  which.max(getHeuristicEntropy(state,ks))\n}\n\ngetLowestExpEntropy <- function(state, ks) {\n  which.min(getExpEntropy(state, ks))\n}\n\n# Correction is based on this graph:\n# plot(log(((0:1000))/sqrt(1000)),sapply(((0:1000)+0)/sqrt(1000),function(x)odds.entropy(exp(log(x)-2))),type='l')\ngetExpEntropy.corrected <- function(state, ks) {\n  state.p <- OddsToP(state)\n  entropy.cor = log(PToOdds(mean(state.p,na.rm=T)))\n  odds.ent.fn = function (x,correct=0) odds.entropy(exp(log(x) - correct))\n  ks.q = length(state)\n  state.t <- sapply(1:ks.q, function(i) ks.update(i, T, state, ks))\n  ent.t <- apply((state.t), 2, odds.ent.fn, correct=entropy.cor)\n  state.f <- sapply(1:ks.q, function(i) ks.update(i, F, state, ks))\n  ent.f <- apply((state.f), 2, odds.ent.fn, correct=entropy.cor)\n  ((ent.t * state.p) + (ent.f * (1 - state.p)))\n}\n\ngetLowestExpEntropy.corrected <- function(state, ks) {\n  which.min(getExpEntropy.corrected(state, ks))\n}\n\nresample <- function(x, size, ...) {\n  if(length(x) <= 1) { if(!missing(size) && size == 0) x[FALSE] else x\n                     } else sample(x, size, ...) }\n\ngetRandom <- function(state,..) {\n  resample(which(!is.na(state)),1)\n}\n\ngetMostConnectedAndUncertainItem <- function(state, ks) {\n  which.min(getConnectedAndUncertainItem(state, ks))\n}\n\ngetHighestEntropyItem <- function(state, ...) {\n  which.min(abs(state-1))\n}\n\ngetHighestEntropyItemUpTo <- function(state, UpTo=length(state)/2, ks) {\n  numberOfNotAns = length(which(state == item.presented))\n  if(numberOfNotAns > UpTo)\n    return(getLowestExpEntropy(state, ks))\n  return(getHighestEntropyItem(state, ks))\n}\n\n# state is in Odds\n# returns a vector instead of saclar like getMostConnectedAndUncertainItem\ngetConnectedAndUncertainItem <- function(state, ks) {\n  ((abs(log(state))+.5) * ks$log.nlinks)\n}\n\ngetNextItem <- function(choice.fn, state, ks, ...) {\n  (choice.fn)(state, ks, ...)\n}\n\n# The option 'by' allows to bypass intermediate points but the performance degrades so it probably should not be used\nsimulation.subj <- function(subj.resp, state, ks, choice.fn, no.propag=F, by=F, verbose=F, estimate.missing=F) {\n  if(verbose) {\n    print('simulation.subj')\n#    if(all(is.na(state))) {\n#      print('all NA in state; entering browser')\n#      browser()\n#    }\n  }\n  n.items <- length(state)\n  result <- state\n  state[is.na(subj.resp)] <- NA\n  ql <- c(0)\n  for(i in 1:n.items) {\n    item <- (choice.fn)(state, ks)        # choose next item according to item selection strategy\n    if(by != F && ((i-1)%%by)!=0) { # simulation by steps of value of 'by'\n      state[item] = item.presented\n    }\n    else if(no.propag || is.na(subj.resp[item]) || item.missing(subj.resp[item]) || length(item)==0)  # third OR clause happens when choice cannot be determined\n      state[item] = item.presented\n    else\n      state <- ks.update(item, subj.resp[item], state, ks)\n    result <- cbind(result,state)\n    ql <- c(ql,item)\n  }\n  # a (logic) bug forces the coercion of the ql to the right size (happens with missing answers),\n  # but given that we do not use the ql it is not important\n  return(data.frame((1:n.items)[1:n.items],result))\n}\n\n# By convention, items less than 0 are missing values\nitem.missing <- function(i) i < 0\nrm.missing <- function(m) { m[item.missing(m)] = NA ; return(m)}\n\n# raw is a matrix [subj,item]\nsimulation <- function(raw, biased=F, p.min=0.5, alpha.c=.25, alpha.p=.5, no.propag=F, choice.fn=getLowestExpEntropy, keep.na=F, ...) {\n  r = NULL\n  ks <- ks.init(raw, p.min=p.min, alpha.c=alpha.c, alpha.p=alpha.p)\n  for (i in 1:nrow(raw)) {\n    if(biased == F && no.propag == F)\n      ks <- ks.init(raw[-i,], p.min=p.min, alpha.c=alpha.c, alpha.p=alpha.p)\n    r <- rbind(r,(simulation.subj(raw[i,], ks$state, ks, choice.fn, no.propag, ...)[,2:(ncol(raw)+2)] > 1)==raw[i,])\n  }\n  r2 <- array(r,c(ncol(raw),nrow(raw),ncol(raw)+1))\n  if(keep.na==F)\n    replace(r2, is.na(r2), 1)\n  else\n    r2\n}\n\n# n: number of runs\n# raw is a matrix [subj,item]\n# n.train : number of training cases (not used with predefined.sampling)\n# predefined.sampling : matrix (items.no, runs)\n# Note : old version had predefined sampling as a vector only\nsimulation.split.n <- function(n, raw, n.train=F, seed=333, predefined.sampling=F, ...) {\n  if(!n.train)\n    n.train = round(nrow(raw) * 0.66)\n  seed.i = seed\n  nsubjs = nrow(raw) - n.train\n  nitems = ncol(raw)\n  if(is.vector(predefined.sampling))\n    aperm(array(sapply(1:n, function(i) simulation.split(raw, n.train, predefined.sampling=predefined.sampling, ...)), c(nitems,nsubjs,nitems+1,n)),c(1,3,2,4))\n  else if(is.matrix(predefined.sampling))\n    aperm(array(sapply(1:n, function(i) simulation.split(raw, n.train, predefined.sampling=predefined.sampling[,i], ...)), c(nitems,nsubjs,nitems+1,n)),c(1,3,2,4))\n  else if(! (predefined.sampling==F))\n    stop(simpleError('predefined.sampling must either be an Nx2 matrix or F'))\n  else {\n    aperm(array(sapply(1:n, function(i) simulation.split(raw, n.train, predefined.sampling=F, ...)), c(nitems,nsubjs,nitems+1,n)),c(1,3,2,4))\n    seed.i = seed.i + 1\n  }\n}\n\n# under development\n# n: number of runs\n# raw.f: matrix without missing values\n# raw.m: matrix with missing values\n# sampling.prob.dist can be a segment of a normal distribution ordered according to the most uncertain items, for eg.:\n# sampling.prob.dist <- dnorm(seq(0,2.5,len=60))[rank(1/abs(colMeans(m2)-.5), ties.method='first')]\nsimulation.split.n.with.missing <- function(n, raw, train.set.size, number.of.missing, sampling.prob.dist=F, ...) {\n  if(sampling.prob.dist==F) {\n    sampling.prob.dist <- NULL;\n  }\n  sampling.prob.dist <- dnorm(seq(0,2.5,len=number.of.missing))[rank(1/abs(colMeans(m2)-.5), ties.method='first')]\n  m2.half.w <- t(apply(m2,1,function(x) {x[sample(1:60,50,prob=sampling.prob.dist)] <- item.presented; return(x)})) # weighted probability of sampling\n\n  train.index <- sample(1:nrow(raw.f), train.set.size, prob=sampling.prob.dist, ties.method='first')\n  test.index <- (1:nrow(raw.f))[-train.index]\n  data.test <- raw.f[test.index,]       # test without missing values\n  data.train <- raw.m[train.index,]     # train with missing values\n  aperm(array(sapply(1:n, function(i)\n                     simulation.cross(data.train, n.train, ...)),\n              c(ncol(raw.f),nrow(raw.f),ncol(raw.f)+1,n)),\n        c(n,3,2,4))\n}\n\n# raw is a matrix [subj,item]\nsimulation.split <- function(raw, n.train, predefined.sampling=NULL, ...) {\n  index.test <- rep(F, nrow(raw))\n  if(is.null(predefined.sampling)) {\n    print('simulation.split')\n    n.test = nrow(raw) - n.train\n    index.test[sample(nrow(raw),n.test)] <- T\n  }\n  else {\n    print('simulation.split w predefined sampling')\n    index.test[predefined.sampling] <- T\n  }\n  index.train <- !index.test\n  raw.train <- raw[index.train,]\n  raw.test <- raw[index.test,]\n  simulation.cross(raw.train, raw.test, ...)\n}\n\n# seed is not used?\n##' <description>\n##'\n##' <details>\n##' @title Perform a cross simulation\n##' @param raw.train Training data\n##' @param raw.test Test data\n##' @param state Allow for providing the state when dealing with missing values\n##' @param p.min minimal probability (ks.init)\n##' @param alpha.c conditional independence tests error (ks.init)\n##' @param alpha.p minimal probability (ks.init)\n##' @param no.propag For benchmarking: no inference\n##' @param choice.fn Passed to ks.update\n##' @param seed used?\n##' @param keep.na If TRUE, will not replace NAs with 1 (allows for correct calculation of accuracy with missing values)\n##' @param answers Can be 'odds' (keeps odds estimate), 'bool' (equivalent to returning odds > 1), or 'valence' (equivalent to (odds > 1) == item outcome)\n##' @param ... \n##' @return An array of [n.items, n.subjs in raw.test, n.items+1 states]\n##' or$t,or$f: odds ratio to compute posterior odds of row given column True,False\n##' m: adjacency matrix where row implies column\n##' @author Michel Desmarais\nsimulation.cross <- function(raw.train, raw.test, state=F, p.min=0.5, alpha.c=.25, alpha.p=.5, no.propag=F, choice.fn=getLowestExpEntropy, seed=NA, keep.na=T, answers='valence', ...) {\n  r = NULL\n  ks <- ks.init(raw.train, p.min=p.min, alpha.c=alpha.c, alpha.p=alpha.p)\n  if(length(state)==1 && state==F)      # allow for providing the state when dealing with missing values\n    state <- ks$state # this can introduce a bias with some types of sampling of question \n  for (i in 1:nrow(raw.test)) {\n    if(answers=='valence') {\n      r0 <- as.matrix((simulation.subj(raw.test[i,], state, ks, choice.fn, no.propag, ...)[,2:(ncol(raw.test)+2)] > 1)==raw.test[i,])\n    } else if(answers=='odds') {\n      r0 <- as.matrix(simulation.subj(raw.test[i,], state, ks, choice.fn, no.propag, ...)[,2:(ncol(raw.test)+2)])\n    } else if(answers=='bool') {\n      r0 <- as.matrix(simulation.subj(raw.test[i,], state, ks, choice.fn, no.propag, ...)[,2:(ncol(raw.test)+2)] > 1)\n    } else {\n      warning('invalid value for answers; using valence')\n      r0 <- as.matrix(simulation.subj(raw.test[i,], state, ks, choice.fn, no.propag, ...)[,2:(ncol(raw.test)+2)] > 1)\n    }\n    r0[is.na(raw.test[i,])] <- missing.item\n    r <- rbind(r, r0)\n  }\n  r2 <- array(as.matrix(r),c(ncol(raw.test),nrow(raw.test),ncol(raw.test)+1))\n  if(keep.na==F)\n    replace(r2, is.na(r2), 1)\n  else\n    r2\n}\n\n#############################################################################\n# Optimized version of 2010.02.24\n#############################################################################\n# Adjacency matrix m is directed row->col\nks.init.o <- function(raw, alpha.c=.25, alpha.p=.25, p.min=.5) {\n  s.less.na <- colSums(!is.na(raw))\n  raw.sum <- colSums(raw, na.rm=T)\n  p <- (raw.sum+1)/(s.less.na+2)\n  odds <- p/(1-p)\n  ans.cp.t <- replace(raw, is.na(raw), 0) # answers for crossprod computations of success\n  ans.cp.f <- round(replace(1-raw, is.na(raw), 0)) # answers for crossprod computations of failures\n  ft <- array(c(crossprod(ans.cp.t,ans.cp.t), # frequency table of TT, TF, FT, and FF\n                                        # f11, f21, f12, f22\n                 crossprod(ans.cp.t,ans.cp.f),\n                 crossprod(ans.cp.f,ans.cp.t),\n                 crossprod(ans.cp.f,ans.cp.f)),\n               c(ncol(ans.cp.t), ncol(ans.cp.t), 4)) + 1 # Laplace correction of + 1\n  condp.t <- (ft[,,1]) / (ft[,,1]+ft[,,3]) # P(row|col)\n  condp.f <- (ft[,,2]) / (ft[,,2]+ft[,,4]) # P(row|!col)\n  odds.t <- Odds(condp.t)                  # O(row|col)\n  odds.f <- Odds(condp.f)                  # O(row|!col)\n  state=odds\n#  or <- list(t=odds.t/odds, f=odds.f/odds) # something to try (doesn't get exactly same result)\n  # Start computing interaction test based on approximation of SE of log.odds.ratio : \\sqrt(\\sum_i 1/n_i)\n  log.odds.ratio <- log((ft[,,1] * ft[,,4])/(ft[,,2] * ft[,,3]))\n  log.odds.se <- sqrt((1/ft[,,1] + 1/ft[,,2] + 1/ft[,,3] + 1/ft[,,4]))\n  log.odds.p <- 2 * pnorm(- abs(log.odds.ratio) / log.odds.se) # two-tail test for a normal distribution\n  # log.odds.interaction is a matrix of the pairs that passed the interaction test\n  log.odds.interaction <- (log.odds.p < alpha.c)\n  m.rel <- log.odds.interaction\n  diag(m.rel) <- F\n  # Compute P(B=1|A=1)\n  a1 <- (ft[,,1]+ft[,,3])-2             # substract Laplace correction\n  b1a1 <- (ft[,,1])-1                   # substract Laplace correction\n  # apply binom.test to slots that passed the interaction test\n  p.b1a1.v <- apply(cbind(b1a1[m.rel], a1[m.rel]),\n                    1,              # by row\n                    function(n.k) pbinom(n.k[1], n.k[2], p.min))\n  # p.b1a1.v is a vector and now we need a matrix \n  p.b1a1 <- matrix(F, ncol(m.rel), ncol(m.rel))\n  # Why is this '>' here and below??  Should be corrected by inverting the ratio.\n  p.b1a1[m.rel] <- p.b1a1.v > alpha.p                 # matrix is re-indexed by m\n  # Repeat for p.a0b0 (P(A=0|B=0)\n  # Compute P(A=0|B=0)\n  a0 <- (ft[,,4]+ft[,,3])-2           # substract Laplace correction\n  a0b0 <- (ft[,,4])-1                 # substract Laplace correction\n  p.a0b0.v <- apply(cbind(a0b0[m.rel], a0[m.rel]),\n                    1,              # by row\n                    function(n.k) pbinom(n.k[1], n.k[2], p.min))\n  # p.a0b0.v is a vector and now we need a matrix \n  p.a0b0 <- matrix(F, ncol(m.rel), ncol(m.rel))\n  p.a0b0[m.rel] <- p.a0b0.v  > alpha.p               # matrix is re-indexed by m\n  # The relation matrix is the combination of both tests (given that the interaction test is\n  # already taken into account) and we put it in integer format for backward compatibility.\n  # Transpose is also for backward compatibility\n  m.rel <- t(round(p.a0b0 & p.b1a1))\n# note: variation qui devrait en theorie etre meilleure mais, en fait, n'apporte aucune difference\n# condp.t <- t(apply(raw,2,function(c) (colSumsRobust(raw[c==1,])+(2*p))/(raw.sum+2) )) # Kononenko (1991) citant Chestnik (1990)\n  or <- list(t=t(m.rel) * odds.t/odds,      # We only retain odds ratios of links and in the next\n                                        # instructions we set the others to 1 such that it has\n                                        # not influence in the computation of new evidence\n                f=m.rel * odds.f/odds)\n  or$t[or$t==0] <- 1                # neutral evidence effect\n  or$f[or$f==0] <- 1                # neutral evidence effect\n  nlinks = colSums(m.rel, na.rm=T) + rowSums(m.rel, na.rm=T)\n  log.nlinks = 0.6931472 / (log((nlinks+1)) + 0.6931472) # 0.6931472 is the entropy of 0.5\n  list(m=m.rel, p=p, odds=odds, condp.t=condp.t, condp.f=condp.f, or=or, state=state,\n       alpha.c=alpha.c, alpha.p=alpha.p, odds.t=odds.t, odds.f=odds.f, or=or, p.min=p.min, nlinks=nlinks, log.nlinks=log.nlinks, nl.for=rowSums(m.rel, na.rm=T), nl.bak=colSums(m.rel, na.rm=T))\n}\n\n# Optimized version used in place of original (currently in poks2-lib.R)\nks.init <- ks.init.o\n\n\n#############################################################################\n## Avoid the problem with variable number of missing values per\n## subject and per item by normalizing\n#############################################################################\n\nsim.normalized.performance <- function(r, ...) {\n if(length(dim(r)) != 3)\n    stop('incorrect argument r (must be [q, nsubjs, q+1]')\n  return(sapply(1:dim(r)[2], function(i) subj.normalized.performance(r[,i,], ...)))\n}\n  \n## subj.normalized.performance <- function(r, ...) {\n##   non.missing <- sum(!is.na(r[,1]))      # non missing will be progressively counted as correct\n##   non.missing.vector <- c(0:non.missing, rep(0, length(r[,1]) - non.missing))\n##   r2 <- (non.missing.vector + colSums(r, na.rm=T))/(non.missing.vector + colSums(!is.na(r)))\n##   approx(r2[!is.nan(r2)], ...)$y\n## }\n# Should be the same as above but simpler\n##' <description>\n##' Normalizes answers by returning an interpolated\n##' vector and by taking into account the missing values.  It asssumes\n##' that  the NA at 0 observation are missing questions and  that all\n##' non missing questions are asked.\n##' <details>\n##' @title Normalized performance of a subject performance\n##' @param r An array that contains correcteness of guesses and having dimension\n##' [n.questions, n.subjects, n.questions+1] which corresponds to\n##' [State 1..n.questions, Subject 1..nsubj, Guess 0..n.questions]\n##' @param ... \n##' @return A vector of length n.questions+1 that averages subject performances\n##' @author Michel Desmarais\nsubj.normalized.performance <- function(r, ...) {\n  q.asked <- !is.na(r[,1])\n  r2 <- r[q.asked,]\n  r2[is.na(r2)] <- 1                    # NA are observed, they are considered correctly classified\n  approx(colMeans(r2)[1:sum(q.asked)], ...)$y\n}\n\n\n#############################################################################\n## Graph\n#############################################################################\n##' <description>\n##'\n##' <details>\n##' @title \n##' @param r Result from simulation.split or simulation.split.n\n##' @param lab Labels\n##' @param col Colors\n##' @param pch Point types\n##' @param title ...\n##' @param linesym ...\n##' @param diag If 'mult', each curve has its own diagonal\n##' @param x.step \n##' @param ystart Defaults to 0.5\n##' @param neg.transform Values of -1 are used for indicating observed and are distinct from missing (NA).  For plotting, they need to be transformed to NA\n##' @param ... Additional parameters passed to 'plot'\n##' @return \n##' @author Michel Desmarais\nplot.sim.results <- function(r, lab=NULL, col = c('blue','magenta','red','green','orange'), pch=c(1,2,8,17,20,18), title='', linesym=F, diag=T, x.step=0, ystart=0.5, neg.transform=TRUE, ...) {\n  if(!is.list(r))\n    r = list(r)\n  r.dim = dim(r[[1]])\n  if(is.null(lab))\n    lab = letters[1:length(r)]\n  ncurves = length(r)\n  if(is.logical(neg.transform) & neg.transform==T & any(sapply(1:ncurves, function(i) any(r[[i]]<0, na.rm=T)))) {\n    for(i in 1:length(r)) {\n      r[[i]][r[[i]]<0] <- NA\n    }\n  }\n  lty = rep(1,length(r))\n  pt.bg = rep(1,length(r))\n  if(!is.vector(linesym) || linesym == F)\n    linesym = rep('b',length(r))\n  # plot item results\n  plotRange = c(ystart,1)\n  x <- seq(0,100,2)\n  plot(x,rep(0,51),type='l',xlab='% Items observed',ylab='Accuracy',main=title,cex=4, ylim=plotRange, ...)\n  abline(h=axTicks(side=2),col=gray(0.7))\n  abline(v=axTicks(side=1),col=gray(0.7))\n  cm <- NULL\n  for(i in ncurves:1) {\n    i.cm <- rowMeans(sim.normalized.performance(r[[i]], n=51))\n    lines(x, i.cm, col=col[i],lty=lty[i],pch=pch[i],type=linesym[i])\n    cm <- cbind(i.cm, cm)\n  }\n  if(diag=='mult')\n    sapply(1:ncurves, function(i) lines(c(0,100),c(mean((r[[i]])[,,1],na.rm=T),1), col='black'))\n  else if(diag)\n    lines(c(0,100),c(i.cm[1],1), col='black')\n  legend(median(axTicks(side=1)),.8,lab,col=col,bg=gray(.95),lty=lty,pch=pch)\n  colnames(cm) <- lab\n  return(cm)\n}  \n\n# Performance as relative gain over baseline\n# r: curves of each simulation line in columns\n# summary: returns details at each interval\nsim.gain <- function(r, summary=T) {\n  intervals <- nrow(r)\n  startv.endv <- cbind(r[1,],r[nrow(r),])\n  baseline <- apply(startv.endv, 1, function(i) approx(i, n=intervals)$y)\n  rel.gain <- (r-baseline)/(1-baseline)\n  rel.gain[is.nan(rel.gain)] <- 1\n  if(summary==T)\n    return(colMeans(rel.gain[-1,]))\n  else\n    return(rel.gain)\n}\n\n\nmake.graphi <- function(r,lab, col = c('blue','magenta','red','green','grey'),pch = c(1,2,8,18),title='', linesym=F, diag=T, x.step=0, ...) {\n  nquest = sum(!is.na((r[[1]])[,1,1]))       # assumes all subjects have same number of questions asked\n  ncurves = length(r)\n  lty = rep(1,length(r))\n  pt.bg = rep(1,length(r))\n  if(!is.vector(linesym) || linesym == F)\n    linesym = rep('b',length(r))\n                                        # plot item results\n#  plotRange = c(ystart,1)\n  plot(0:nquest,rep(0,nquest+1),type='b',xlab='Item',ylab='Accuracy',main=title,cex=4, ...)\n  abline(h=axTicks(side=2),col=gray(0.7))\n  abline(v=axTicks(side=1),col=gray(0.7))\n  if(x.step != 0)\n    x.ind = (((0:nquest) %% x.step)==0)\n  else\n    x.ind = rep(T,nquest)\n  for(i in ncurves:1) {\n    i.cm <- colMeans2(r[[i]],na.rm=T)[1:(nquest+1)]\n    lines((0:nquest)[x.ind],(i.cm)[x.ind],col=col[i],lty=lty[i],pch=pch[i],type=linesym[i])\n  }\n  if(diag)\n    lines(c(0,nquest),c(i.cm[1],1), col='black')\n  legend(median(axTicks(side=1)),.8,lab,col=col,bg=gray(.95),lty=lty,pch=pch)\n}\n\n## Version that assumes NAs in r.w.na[,,1] indicate missing items; also\n## allows more flexibility for other options\n\nmake.graphi.na <- function(r.w.na,lab, col = NULL, pch = NULL,title='', linesym=F, diag=T, x.step=0, draw.0r=F, ...) {\n  r <- sapply(r.w.na, filter.na)\n  nquest = sum(!is.na((r[[1]])[,1,1]))\n  if(any(apply((r[[1]])[,,1], 2, function(i) sum(!is.na(i))) != nquest))\n    stop('make.graphi.na: assumes all subjects have same number of questions asked')\n  ncurves = length(r)\n  lty = rep(1,length(r))\n  pt.bg = rep(1,length(r))\n  if(!is.vector(linesym) || linesym == F)\n    linesym = rep('b',length(r))\n                                        # plot item results\n#  plotRange = c(ystart,1)\n  plot(0:nquest,rep(0,nquest+1),type='b',xlab='Item',ylab='Accuracy',main=title,cex=4, ...)\n  abline(h=axTicks(side=2),col=gray(0.7))\n  abline(v=axTicks(side=1),col=gray(0.7))\n  if(x.step != 0)\n    x.ind = (((0:nquest) %% x.step)==0)\n  else\n    x.ind = rep(T,nquest)\n  if(is.null(col))\n    col = c('blue','magenta','red','green','grey','blue2','magenta2','red2','green2','grey2')[1:ncurves]\n  if(is.null(pch))\n    pch = c(1,2,8,3,4,5,6,7,9,1,2,8,3,4,5,6,7,9)[1:ncurves]\n  for(i in ncurves:1) {\n    i.cm <- colMeans2(r[[i]],na.rm=T)[1:(nquest+1)]\n    lines((0:nquest)[x.ind],(i.cm)[x.ind],col=col[i],lty=lty[i],pch=pch[i],type=linesym[i])\n  }\n  if(diag)\n    lines(c(0,nquest),c(i.cm[1],1), col='black')\n  if(draw.0r) {\n    r0 <- array((r[[1]])[,,1], dim(r[[1]]))\n    r0[is.na(r.w.na[[1]])] <- NA\n    lines((0:nquest)[x.ind],colMeans2(filter.na(r0)[[1]],na.rm=T)[x.ind],col='black',lty=1,pch=0)\n    lab <- c(lab, '0r'); col <- c(col, 'black'); lty <- c(lty, 1); pch <- c(pch, -1)\n  }\n  legend(median(axTicks(side=1)),.8,lab,col=col,bg=gray(.95),lty=lty,pch=pch)\n}\n\n#############################################################################\n# Standard deviation routines\n#############################################################################\nscore <- function(a) {\n  r.exp.cor = round(a)\n  for (i in 1:ncol(a)) r.exp.cor[,i,,] = r.exp.cor[,ncol(a),,]\n  round(r.exp.cor == round(a))\n}\n\n# r is a matrix [item,subj,sequence,run] that has been already scored: score(a)\nplot.ci.boot <- function(r) {\n  nnodes = nrow(r)\n  r.subj=(sapply(1:(nnodes+1),function(i)colMeans(r[,i,,])))\n  r.subj.sd = sd(r.subj)\n  if(any(r.subj.sd == 0))\n    r.subj = r.subj[,1:(which(r.subj.sd == 0)[1]-1)]\n  # compute confidence intervals\n  r.ci = sapply(1:ncol(r.subj),function(i) boot.ci(boot(r.subj[,i],function(x,i)mean(x[i]),R=1000),conf=.95,type=c('perc'))$percent)[4:5,]\n  if(any(r.subj.sd == 0))\n    r.ci = cbind(r.ci,matrix(0,2,(nnodes-(which(r.subj.sd == 0)[1] - 2))))\n  arrows(0:(nnodes-1), r.ci[1,], 0:(nnodes-1), r.ci[2,], code=3, angle=90, length=.02)\n}\n\n# r is a matrix [item,sequence,subj,run] that has been already scored: score(a)\nsd.norm <- function(r, alpha=.9, na.rm=F) {\n  sd(t(sapply(1:dim(r)[3],function(j) rowMeans(sapply(1:dim(r)[4],function(i)colMeans(r[,,j,i],, na.rm=na.rm)))))) * qnorm(alpha)\n}\n\n# not used and unverified\nsd.norm.btw.runs <- function(r, alpha=.9) {\n  sd(colMeans(aperm(sumRuns(r),c(3,1,2)))) * qnorm(alpha)\n}\n\nsumRuns <- function(r) {\n  accum = r[,,,1]\n  for(i in 2:(dim(r)[4])) {\n    accum = accum + r[,,,i]\n  }\n  accum / (dim(r)[4])\n}\n\nplot.ci.norm <- function(r, alpha=.9, offset=0, na.rm=F, ...) {\n  curve = rowMeans(colMeans(score(r)),na.rm=na.rm)\n  sd = sd.norm(r, alpha, na.rm=na.rm)\n  nnodes = length(sd)\n  high = curve+sd\n  low = curve-sd\n  high[which(high>1)] = trunc(high[which(high>1)])\n  r.ci = rbind(high, low)\n  arrows((0:(nnodes-1))+offset, r.ci[1,], (0:(nnodes-1))+offset, r.ci[2,], code=3, angle=90, length=.02, ...)\n}\n\n# assume r is scored\n# plot.ci.per.sim <- function(r, alpha=.9, offset=0, ...) {\n#   curves = colMeans(aperm(colMeans(r),c(2,3,1)))\n#   mean.curve = colMeans(curves)\n#   sd = sd(curves) * qnorm(alpha)\n#   nnodes = length(sd)\n#   high = mean.curve+sd\n#   low = mean.curve-sd\n#   high[which(high>1)] = trunc(high[which(high>1)])\n#   r.ci = rbind(high, low)\n#   arrows((0:(nnodes-1))+offset, r.ci[1,], (0:(nnodes-1))+offset, r.ci[2,], code=3, angle=90, length=.02, ...)\n# }\n\nplot.ci.per.sim <- function(r, alpha=.9, offset=0, step=1, ...) {\n  curves = colMeans(aperm(colMeans(r),c(2,3,1)))\n  mean.curve = colMeans(curves)\n  sd = sd(curves) * qnorm(alpha)\n  nnodes = length(sd)\n  high = mean.curve+sd\n  low = mean.curve-sd\n  high[which(high>1)] = trunc(high[which(high>1)])\n  r.ci = rbind(high, low)\n  seq.step <- seq(1,(nnodes),step)\n  arrows(((0:(nnodes-1))+offset)[seq.step], (r.ci[1,])[seq.step], ((0:(nnodes-1))+offset)[seq.step], (r.ci[2,])[seq.step], code=3, angle=90, length=.02, ...)\n}\n\np.to.star <- function(p.vec) {r = rep('-',length(p.vec)); r[p.vec<.05]='*'; r[p.vec<.01]='**'; r[p.vec<.001]='***';return(r);}\n\n#############################################################################\n## Generation of simulated data\n#############################################################################\ntf.sample <- function(n,p) {\n    sapply(p, function(pi) sample(c(T,F),n,replace=T,prob=c(pi,1-pi)))\n}\n\n## Returns a [Subject X Concept] mastery matrix.\n## NAs are removed for computing mastery.\n## Consider u.topic.mastery as a skill specific slip parameter\ntopic.mastery <- function(data, qm, no.correction=F) {\n  n.items <- ncol(data)\n  data2 <- data\n  data2[is.na(data)] <- 0\n  ## Todo: to compensate for NA : must build a matrix instead of colSums\n  ## where incidences of NAs are deducted for each subject in data\n  mastery <- t((data2 %*% qm)/topics.na.rm(data, qm))\n  if(no.correction)\n    { return(mastery) }\n  else\n    { return((t(data2 %*% qm)+rowMeans(mastery))/(c(rep(1,n.items) %*% qm)+1)) } \n}\n\n## For each NA in a subject row that affects concept computation,\n## deduct the number of occurences.  Allows the computation of concept\n## mastery while removing NAs.\ntopics.na.rm <- function(data, qm) {\n  qm.cs <- colSums(qm)\n  na.incidence <- is.na(data) %*% qm\n  r <- qm.cs - na.incidence\n  r[r==0] <- -1                         # to avoid 0 division\n  return(r)\n}\n\nidentity <- function(x) x\n\n## Generate data based on topic mastery\n## 'data': matrix: Subjs X Item;  and can contain NA\n## 'qm' : matrix: Item X Topic (created with topic.mastery())\ngen.simulated.data <- function(data, qm) {\n  topic.mastery <- topic.mastery(data, qm)\n  q.prob <- sapply(1:ncol(data), function(i) apply(matrix(topic.mastery[qm[i,]>0,], ncol=nrow(data)), 2, min))\n  data.na.ind <- is.na(data)\n  sim.qm.seq <- sample(which(!data.na.ind), trunc(mean(data,na.rm=T) * sum(!data.na.ind)), prob=q.prob[!data.na.ind])\n  data.sim <- matrix(0, nrow(data), ncol(data))\n  data.sim[data.na.ind] <- NA\n  data.sim[sim.qm.seq] <- 1\n  return(data.sim)\n}\n\n# Same as gen.simulated.data but withouth concepts\ngen.simulated.expected <- function(data) {\n  rp <- (rowMeans(data,na.rm=T)+1)/(ncol(data)+2)\n  cp <- (colMeans(data,na.rm=T)+1)/(nrow(data)+2)\n  ex <- (rp %o% cp) * sum(data,na.rm=T) # expected probability\n  data.na.ind <- !is.na(data)\n  sim.seq <- sample(which(data.na.ind), trunc(mean(data,na.rm=T) * sum(data.na.ind)), prob=c(ex)[data.na.ind])\n  data.sim <- matrix(0, nrow(data), ncol(data))\n  data.sim[!data.na.ind] <- NA\n  data.sim[sim.seq] <- 1\n  return(data.sim)\n}\n# Same as gen.simulated.data keeping exact row distribution (transpose for obtaning exact same column dist)\ngen.simulated.expected.r <- function(data) {\n  cp <- (colMeans(data,na.rm=T)+1)/(nrow(data)+2)\n  rs <- rowSums(data,na.rm=T)\n  data.na.ind <- !is.na(data)\n  r <- sapply(1:nrow(data), function(i) {\n    ro.seq <- sample(which(data.na.ind[i,]), rs[i], prob=cp[data.na.ind[i,]])\n    ro.sim <- rep(0, ncol(data))\n    ro.sim[!data.na.ind[i,]] <- NA\n    ro.sim[ro.seq] <- 1\n    return(ro.sim)\n  })\n  return(t(r))\n}\n\n#plot(dnorm((-40:40)/10))\n#lines(dlogis(1.7*(-40:40)/10)*.4/.25)\n#lines(dlogis(1*(-40:40)/10)*.4/.25)\n#############################################################################\n## IRT simulated data\n# d.examinee: vector of examinee params (thetas)\n# d.items: matrix of item params (difficulty, discrimination)\n# To test with UNIX data: sim.compare(gen.simulated.irt(u.irt.e, u.irt.i), u.m, u.qm)\n# (should yield very row and column correlations)\ngen.simulated.irt <- function(d.examinee, d.items) {\n  t(round(sapply(d.examinee, function(j) apply(d.items, 1, function(i) rlogis(1, j-i[2], 1/i[1]))) > 0))\n}\n\n#############################################################################\n# Generate simulated data with \n# data is a {0,1} matrix Examinee X Item\ncovariance.sim.data <- function(data, fit.rows=F) {\n  data.cv <- cov(data)                  # Covariance matrix\n  # Upper triangular factor of the Cholesky decomposition\n  L <- chol(data.cv)                    \n  # Normalized simulated sample\n  temp.data.sim <- matrix(rnorm(ncol(data)*nrow(data)), nrow=nrow(data)) %*% L\n  # Add item average success rate and perform discrete transformation\n  data.sim  <- sweep(temp.data.sim, 2, colMeans(data)-0.5, '+')\n  if(fit.rows==T)\n    data.sim  <- sweep(data.sim, 1, rowMeans(data)-0.5, '+')\n  return(data.sim > 0)\n}\n\n#############################################################################\n## Simulated data routines with POKS\n\n# Generate a single sample of data from the predefined 'ks'.\n# I believe this corresponds to a Gibbs sampler.\n\n# 'burn' indicates the number of initial values to set without taking\n# into account observed nodes\nGenSamplePOKS <- function(ks, burn=2) {\n  n.q <- ncol(ks$m)\n  qi <- 1:n.q\n  s=rep(NA,n.q)\n  n.t <- trunc(burn/2)\n  n.f <- trunc(burn/2)\n#  s[sample(qi, n.t, prob = ks$p)] <- T\n#  s[sample(qi, n.f, prob = 1 - ks$p)] <- F\n  s[sample(qi, n.t)] <- T\n  s[sample(qi, n.f)] <- F\n  for(ignore in 1:(n.q-burn)) {\n    na.i <- is.na(s)\n    if(sum(na.i)==1)                    # otherwise sample behaves the wrong way\n      i=qi[na.i]\n    else\n      i=sample(qi[na.i],1)\n    if(!is.na(s[i])) stop(paste('i NA?? Step', ignore))\n    p=ProbGObs(i, s, ks)\n    s[i] = sample(c(T,F), 1, prob=c(p,1-p))\n  }\n  return(s)\n}\n## Returns the probability of 'node=T' given a set of existing 'obs' and the predefined 'ks'\nProbGObs <- function(node, obs, ks) {\n  OddsToP(prod(ks$state[node],\n               ks$or$t[node,(!is.na(obs) & obs)],\n               ks$or$f[node,(!is.na(obs) & !obs)],\n               na.rm=T))\n}\n\nsim.compare <- function(data,sim, qm=F) {\n  print('general means, Correlation row means, Correlation col means, Correlation concept means on respondant basis')\n  if(!is.matrix(qm))\n    qm <- matrix(1,ncol(data),2)        # make a dummy Q-matrix\n  data.frame('Mean data'=mean(data,na.rm=T),'Mean sim'=mean(sim,na.rm=T),\n    'Cor Row'=cor(rowMeans(data,na.rm=T),rowMeans(sim,na.rm=T)),\n    'Cor Col'=cor(colMeans(data,na.rm=T),colMeans(sim,na.rm=T),use='complete.obs'),\n    'Col Concepts'=cor(c(topic.mastery(data, qm)), c(topic.mastery(sim, qm))),\n    '%diff'= mean(data==sim)\n             )\n}\n\n## Transform missing values which show up at 0 observations as NA into\n## -1 in the results.  This allows to distinguish them from observed\n## items which are marked with NA after 0 observations\nmissing.to.neg <- function(results) {\n  r <- results\n  ## Results[,,1] is state after 0 observations and thus contains NA\n  ## where missing data is.  This pattern is repeated over all\n  ## observations and thus repeated n-1 times the last dimension in the array.\n  r[rep(is.na(results[,,1]), (dim(results)[3]-1))] <- -1\n  return(r)\n}\n## Follows the transformation of missing.to.neg: NA are considered\n## observed and set to 1, and -1 are put to NA\nna.to.obs <- function(results) {\n  r <- results\n  r[is.na(results)] <- 1\n  r[results<0] <- NA\n  return(r)\n}\n\n## Combines both routines na.to.obs and missing.to.neg\nfilter.na <- function(...) sapply(list(...), function(m) list(na.to.obs(missing.to.neg(m))))\n\n## Extracts the position where each item is asked in a simulation run\n## based on the first appearance of NA\nextract.item.pos <- function(r) {\n  t(sapply(1:(dim(r)[2]),\n         function(i) apply(r[,i,], 1, function(i) match(NA,i)))) - 1\n}\n\n## Normalize results to equal length by interpolation.\nnormalize.res <- function(r, len=100) {\n  rn <- aggregate.res.per.item(r)\n  apply(rn, 2, function(i) { n.valid=(length(i)-sum(is.nan(i))); interpolate.vector(i[1:n.valid], len) })\n}  \n\n## Aggregate results of simulation data with missing items. \naggregate.res.per.item <- function (r) apply(r, 2, function(i) { r.mis=is.na(i[,1]); colMeans(i, na.rm=T) })\n\n## Aggregate results of simulation data with missing items, counting\n## observed for correct. It allows for subjects to have different\n## numbers of items, which implies a normalization step to obtain a\n## matrix of equal sequenced observations.\n## Eg. plot(rowMeans(aggregate.res.per.item.obsT(r)))\naggregate.res.per.item.obsT <- function (r, step=100) {\n  apply(r, 2,\n        function(i) {\n          r.mis=is.na(i[,1]);\n          i2=i[!r.mis,];\n          subj.r <- apply(i2, 2, function(j) {\n            j[is.na(j)] <- 1\n            mean(j)\n          })\n          n.valid=match(T,apply(i2,2,function(i) all(is.na(i))))\n          interpolate.vector(subj.r[1:n.valid], step)\n        })\n}\n\n## Interpolate a vector to a new vector of given length (assumes constant spacing)\ninterpolate.vector <- function (v, len)   approx(1:length(v), v, seq(1,length(v),length.out=len))$y\n\n## Returns residual score of variable length according to missing values; need\n## to turn it into normalized lenght and replace residual with score\n## that consider oberved as correct.  Could also use normalized length\n## as a good score.\n\n\n\n\n#############################################################################\n## IRT simulation with 'ltm' package\n#############################################################################\n\n## Replicate a simulation run (by POKS) with IRT from 'ltm' package\n## DOES NOT GENERATE FAITHFUL DATA as does gen.simulated.irt\n## m is the response matrix\n## res.repl is the output of 'simulation' or 'simulation.split' (keeping NA?)\n## If 'model' is specified, it must have been specified as (<data> ~ z1)\n## require(multicore)\nirt.sim.repl <- function(m, res.repl, model=NULL, no.bias=F, cap.coefs=T, no.multicore=F) {\n  require(ltm)\n  if(is.null(model) & no.bias==F)\n    model <- ltm(m ~ z1)\n  rp <- array(t(m), dim(res.repl))\n  rp[!is.na(res.repl)] <- NA\n  if(no.multicore == T)\n    mclapply <- lapply\n  r <- mclapply(1:dim(res.repl)[3], function(i) {\n    if(no.bias == T)\n      model = ltm(m[-i,] ~ z1)\n    fs <- factor.scores(model, resp.pattern=t(rp[,,i]))\n    if(cap.coefs == T) {\n      disc = cap(coef(model)[,2], 0, 4)\n      diffc = cap(coef(model)[,1], -4, 4)\n    } else {\n      disc = coef(model)[,2]\n      diffc = coef(model)[,1]\n    }\n    sapply(1:dim(res.repl)[1], function(j)\n           icc.p(fs$score.dat$z1,disc[j],diffc[j]))\n  })\n  return(irt.r.filter(unlist(r), m, res.repl))\n}\n  \ncap <- function(x,min,max) pmax(pmin(x,max),min)\n\n## Utility function of irt.sim.repl to process the intermediate result data\nirt.r.filter <- function(r, r.m, r.repl) {\n  nquest <- dim(r.repl)[1]\n  nsubj <- dim(r.repl)[2]\n  r2 <- ((r>.5) == rep(c(r.m), nquest+1))\n  r3 <- aperm(array(r2, c(nsubj,nquest,nquest+1)),c(2,1,3))\n  r3[is.na(r.repl)] <- NA\n  return(r3)\n}\n\nset <- function(v, val, ind) {v[ind] <- val; v}\n",
    "created" : 1437599366331.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "967745006",
    "id" : "BFFFA8DD",
    "lastKnownWriteTime" : 1438135141,
    "path" : "~/POKS-skills/lib-poks.R",
    "project_path" : "lib-poks.R",
    "properties" : {
    },
    "relative_order" : 2,
    "source_on_save" : false,
    "type" : "r_source"
}